# Gemini Robotics ER 自律制御システム 詳細仕様書

## 1. 概要
本システムは、Gemini Robotics-ER モデルを活用し、自然言語による指示とカメラ画像のみを用いて、シミュレーション上のロボットを自律制御するダッシュボードアプリケーションです。

## 2. 基本方針
*   **完全自律性**: ロボットは自身の搭載カメラ（Robot View）とユーザーからの自然言語指示のみを入力とし、環境の内部状態（絶対座標やオブジェクトの真の位置など）には依存せずに動作します。
*   **汎用プランニング**: 「赤いポールのところへ行って」といった抽象的な指示を、具体的な行動シーケンス（旋回、移動、探索など）に分解して実行します。現行実装は「1アクションずつ計画→実行→再計画」のループで動作します。
*   **リアルタイム可視化**: ユーザー向けに、ロボットの視点だけでなく、環境全体を俯瞰する視点（Global View）と、現在のロボットの内部状態を提供します。

## 3. システムアーキテクチャ

```mermaid
graph TD
    User[ユーザー] -->|自然言語指示| Dashboard[Dashboard (UI)]
    Dashboard -->|指示 + 現在の画像| Planner[Agent Planner (Gemini Flash)]
    Planner -->|タスクリスト (JSON)| TaskManager[Task Manager]
    
    subgraph Execution Loop
        TaskManager -->|現在のタスク| Executor[Executor]
        Executor -->|画像 + ターゲット| Vision[Vision System (Gemini ER)]
        Vision -->|バウンディングボックス / 状態| Controller[Robot Controller]
        Controller -->|モーター指令| Sim[MuJoCo Simulation]
        Sim -->|新しい画像| Executor
    end
```

## 4. 機能要件

### 4.1. ユーザーインターフェース (UI)
*   **Global View (俯瞰カメラ)**:
    *   ユーザーが状況を把握するための全体映像。ロボットの自律制御には使用しない。
*   **Robot View (ロボット視点)**:
    *   ロボットが「見ている」映像。プランニングと物体認識の入力として使用。
    *   認識結果（バウンディングボックス）をオーバーレイ表示。
*   **Status Panel (ステータス表示)**:
    *   現在のタスク内容（例: "Searching for red pole"）。
    *   実行状態（RUNNING / STOPPED）。
    *   モーター出力値、ロボットの推定位置（デバッグ用）。
*   **Control Panel (操作パネル)**:
    *   **Instruction Input**: 自然言語で指示を入力するテキストボックス。
    *   **Plan & Run**: 入力された指示に基づき計画を作成し、実行を開始するボタン。
    *   **Scene Selection**: 異なる環境（障害物あり、ターゲット配置違いなど）をロードするドロップダウンリスト。
    *   **Emergency Stop**: 即座にロボットを停止させるボタン。

### 4.2. Agent Planner (計画生成)
*   **入力**: 自然言語指示、現在の Robot View 画像、必要に応じて直近の周囲4方向画像。
*   **出力**: その時点での **単一ステップ** アクション（JSON形式）。
*   **対応アクション（現行）**:
    *   `scan`: その場で360度スキャン（右回り90度×3回で4ビュー取得）。
    *   `turn_left_30` / `turn_right_30`: 約30度の旋回。
    *   `move_forward_short`: 短い前進。
    *   `approach`: 対象物へビジュアルサーボ。
    *   `stop`: 停止。
*   **プランニングルールの要点**:
    *   周囲画像が無い場合はまず `scan`。
    *   周囲画像で対象が見えている方向へ旋回。
    *   現在視野に対象があれば `approach`。
    *   見えていない場合は前進や再スキャンで探索を継続。
*   **メモ**: スキャン実行中は追加のプランニングを行わず、4ビュー取得完了後に再プランする。

### 4.3. Vision System (視覚認識)
*   **モデル**: `gemini-robotics-er-1.5-preview` (または最新のRobotics向けモデル)。
*   **機能**:
    *   指定された `target` を画像内から検出。
    *   2Dバウンディングボックス `[ymin, xmin, ymax, xmax]` を返却。
    *   対象が見つからない場合は `null` を返却。

### 4.4. Robot Controller (制御)
*   **Open-Loop制御**:
    *   離散アクション（`turn_left_30` / `turn_right_30` / `move_forward_short`）は固定速度・固定時間で実行。
    *   スキャン時の90度旋回は `SCAN_TURN_SPEED` ≈ 3.5, `SCAN_TURN_DURATION` ≈ 4.0 秒で右回りを3回行い全周取得。
*   **Closed-Loop制御 (Visual Servoing)**:
    *   `approach` は 1.5 秒間隔で Vision bbox を連続ポーリングし、結果が届くたびにモーター指令を更新し続ける。
    *   到着判定: bbox 高さ > 950 かつ 水平中心が画面中心±120px で左右端から十分離れている場合。
    *   近距離では段階的に減速（高さ700/850で速度低減）、端寄り時は旋回ゲインを下げ速度を抑制し暴れを防止。旋回出力はクランプ。

## 5. シーン管理
*   **構成ファイル**: MuJoCoのXML形式 (`scene.xml`) で定義。
*   **バリエーション**:
    1.  **Basic**: 何もない空間にターゲット（ポール）のみ。
    2.  **Obstacle**: ターゲットの手前に障害物（青いブロックなど）がある。
    3.  **Complex**: 複数のターゲットと障害物が配置された迷路的な環境。
*   **ランダム化**: ボタン一つでターゲットやロボットの初期位置をランダムに変更可能にする。

## 6. 今後の拡張性
*   **失敗検知と再計画**: タスク実行中にターゲットを見失った場合や、長時間進捗がない場合に、自動的にPlannerを呼び出して計画を修正する機能。
*   **マルチモーダルフィードバック**: ロボットが「見つかりません」「到着しました」などをテキストまたは音声でユーザーに伝える機能。

## 7. デバッグ
*   `ER_DEBUG=1` 環境変数でデバッグモードを有効化。
*   アクション開始/終了、ビジョン更新ごとに `[DBG] action=... motors=... dur=... pos=... yaw=... bbox=...` を UI ログとコンソールへ出力し、指示と実行の突き合わせを容易にする。
