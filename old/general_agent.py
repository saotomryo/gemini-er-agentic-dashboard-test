import mujoco
import numpy as np
import cv2
import time
import json
import os
from dotenv import load_dotenv
import google.generativeai as genai
from PIL import Image

load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# ---------------------------------------------------------
# 1. Planner: è‡ªç„¶è¨€èªã‚’ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã«åˆ†è§£ã™ã‚‹
# ---------------------------------------------------------
class TaskPlanner:
    def __init__(self):
        # ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã«ã¯å®‰ä¾¡ã§é«˜é€ŸãªFlashã‚’ä½¿ç”¨
        self.model = genai.GenerativeModel('gemini-2.5-flash')

    def plan_tasks(self, instruction):
        """
        å…¥åŠ›: "èµ¤ã„ãƒãƒ¼ãƒ«ã«è¡Œã£ã¦ã‹ã‚‰ã€é’ã„ç®±ã‚’å‘ã„ã¦"
        å‡ºåŠ›: [{"action": "move_to", "target": "red pole"}, {"action": "look_at", "target": "blue box"}]
        """
        prompt = f"""
        You are a robot instruction parser. Convert the following natural language command into a sequence of tasks.
        
        Command: "{instruction}"
        
        Available actions:
        - "move_to": Approach the target until close.
        - "look_at": Turn towards the target but do not approach closely.
        
        Output JSON format:
        [
          {{"action": "move_to", "target": "description of object"}},
          ...
        ]
        Return ONLY the JSON array.
        """
        response = self.model.generate_content(prompt)
        try:
            text = response.text.replace("```json", "").replace("```", "").strip()
            return json.loads(text)
        except:
            print("âš ï¸ è¨ˆç”»ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            return []

# ---------------------------------------------------------
# 2. Vision: æŒ‡å®šã•ã‚ŒãŸç‰©ä½“ã‚’æ¢ã™ (æ±ç”¨)
# ---------------------------------------------------------
class VisionSystem:
    def __init__(self):
        # ç©ºé–“èªè­˜ã«ã¯ Robotics-ER (ãªã‘ã‚Œã°Pro/Flash)
        model_name = 'models/gemini-robotics-er-1.5-preview'
        # model_name = 'gemini-1.5-flash' # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã“ã¡ã‚‰
        self.model = genai.GenerativeModel(model_name)

    def detect_object(self, img_array, target_description):
        """
        ç”»åƒã¨ã€Œæ¢ã™ã¹ãã‚‚ã®ã®åå‰ã€ã‚’å—ã‘å–ã‚Šã€åº§æ¨™ã‚’è¿”ã™
        """
        prompt = f"""
        Detect the "{target_description}" in the image.
        Return the 2D bounding box in JSON format with keys "box_2d" [ymin, xmin, ymax, xmax].
        If not found, return null.
        """
        try:
            pil_img = Image.fromarray(img_array)
            response = self.model.generate_content(
                [prompt, pil_img],
                generation_config={"response_mime_type": "application/json"}
            )
            data = json.loads(response.text)
            if "box_2d" in data and data["box_2d"]:
                return data["box_2d"]
            return None
        except Exception as e:
            return None

# ---------------------------------------------------------
# 3. Controller: åº§æ¨™ã‚’ãƒ¢ãƒ¼ã‚¿ãƒ¼æŒ‡ä»¤ã«å¤‰æ›ã™ã‚‹
# ---------------------------------------------------------
class RobotController:
    def __init__(self):
        self.screen_center = 500 # 0-1000ã®ä¸­å¤®
        self.kp_turn = 0.05      # æ—‹å›ã‚²ã‚¤ãƒ³
    
    def calculate_command(self, bbox, action_type):
        """
        åº§æ¨™ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—("move_to" or "look_at")ã‹ã‚‰é€Ÿåº¦ã‚’è¨ˆç®—
        æˆ»ã‚Šå€¤: (å·¦å³ãƒ¢ãƒ¼ã‚¿ãƒ¼é€Ÿåº¦, å®Œäº†ã—ãŸã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°)
        """
        if not bbox:
            return [5.0, -5.0], False # è¦‹ã¤ã‹ã‚‰ãªã„ãªã‚‰æ—‹å›æ¢ç´¢

        ymin, xmin, ymax, xmax = bbox
        center_x = (xmin + xmax) / 2.0
        height = ymax - ymin # ç‰©ä½“ã®å¤§ãã•(è¿‘ã•)

        # ç”»é¢ä¸­å¤®ã¨ã®ã‚ºãƒ¬
        error_x = center_x - self.screen_center
        turn = error_x * self.kp_turn

        # --- ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ã®åˆ¶å¾¡ ---
        
        # A. è¿‘ã¥ã (Move To)
        if action_type == "move_to":
            if height > 850: # ååˆ†è¿‘ã¥ã„ãŸã‚‰å®Œäº†
                return [0.0, 0.0], True
            
            # å‰é€² + æ—‹å›
            base_speed = -15.0
            return [base_speed + turn, base_speed - turn], False

        # B. å‘ãã ã‘ (Look At)
        elif action_type == "look_at":
            # ä¸­å¤®ä»˜è¿‘ã«æ¥ãŸã‚‰å®Œäº†
            if abs(error_x) < 50: 
                return [0.0, 0.0], True
            
            # ãã®å ´æ—‹å›ã®ã¿
            return [turn, -turn], False
        
        return [0.0, 0.0], False

# ---------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨
# ---------------------------------------------------------
def main():
    # å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    planner = TaskPlanner()
    vision = VisionSystem()
    controller = RobotController()
    
    # --- ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿è¨­å®š ---
    model_mj = mujoco.MjModel.from_xml_path('scene.xml')
    data_mj = mujoco.MjData(model_mj)
    renderer = mujoco.Renderer(model_mj, height=240, width=320)
    global_renderer = mujoco.Renderer(model_mj, height=480, width=640)

    # ==========================================
    # â˜… ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æŒ‡ç¤º (ã“ã“ã‚’å¤‰ãˆã‚‹ã¨å‹•ããŒå¤‰ã‚ã‚‹)
    # ==========================================
    user_instruction = "èµ¤ã„ãƒãƒ¼ãƒ«ã¾ã§ç§»å‹•ã—ã¦ã€‚ãã®ã‚ã¨ã€é’ã„ç®±ã®æ–¹ã‚’å‘ã„ã¦ã€‚"
    
    print(f"ğŸ—£ï¸ æŒ‡ç¤º: {user_instruction}")
    print("ğŸ§  è¨ˆç”»ä¸­...")
    task_queue = planner.plan_tasks(user_instruction)
    print(f"ğŸ“‹ ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ: {json.dumps(task_queue, indent=2, ensure_ascii=False)}")
    
    # å®Ÿè¡Œãƒ«ãƒ¼ãƒ—ç”¨å¤‰æ•°
    current_task_idx = 0
    last_api_time = 0
    current_ctrl = [0.0, 0.0]
    bbox_display = None
    
    step = 0
    while True:
        # --- ã‚¿ã‚¹ã‚¯ç®¡ç† ---
        if current_task_idx < len(task_queue):
            task = task_queue[current_task_idx]
        else:
            print("ğŸ‰ å…¨ã‚¿ã‚¹ã‚¯å®Œäº†ï¼")
            break # ãƒ«ãƒ¼ãƒ—çµ‚äº†

        # --- ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ ---
        data_mj.ctrl[0] = current_ctrl[1]
        data_mj.ctrl[1] = current_ctrl[0]
        mujoco.mj_step(model_mj, data_mj)
        step += 1

        # --- AIå‡¦ç† (é–“éš”ã‚’ç©ºã‘ã¦å®Ÿè¡Œ) ---
        if time.time() - last_api_time > 1.5:
            
            renderer.update_scene(data_mj, camera="robot_cam")
            img = renderer.render()
            
            # 1. ä»Šã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’æ¢ã™
            target_name = task["target"]
            print(f"ğŸ‘ï¸ æ¢ç´¢ä¸­: {target_name} ({task['action']})...", end="\r")
            bbox_display = vision.detect_object(img, target_name)
            
            # 2. ãƒ¢ãƒ¼ã‚¿ãƒ¼æŒ‡ä»¤ã‚’è¨ˆç®— & å®Œäº†åˆ¤å®š
            new_ctrl, is_done = controller.calculate_command(bbox_display, task["action"])
            current_ctrl = np.clip(new_ctrl, -20, 20) # ã‚¯ãƒªãƒƒãƒ—
            
            if is_done:
                print(f"\nâœ… ã‚¿ã‚¹ã‚¯å®Œäº†: {target_name}")
                current_task_idx += 1
                current_ctrl = [0.0, 0.0] # ä¸€æ—¦åœæ­¢
                bbox_display = None
                time.sleep(1) # ã‚ã‹ã‚Šã‚„ã™ãå°‘ã—å¾…ã¤

            last_api_time = time.time()

        # --- ç”»é¢æç”» ---
        if step % 5 == 0:
            renderer.update_scene(data_mj, camera="robot_cam")
            img_bgr = cv2.cvtColor(renderer.render(), cv2.COLOR_RGB2BGR)
            
            # èªè­˜æ ã®è¡¨ç¤º
            if bbox_display:
                h, w, _ = img_bgr.shape
                ymin, xmin, ymax, xmax = bbox_display
                cv2.rectangle(img_bgr, (int(xmin/1000*w), int(ymin/1000*h)), (int(xmax/1000*w), int(ymax/1000*h)), (0, 255, 0), 2)
                cv2.putText(img_bgr, task["target"], (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)

            cv2.imshow('Robot Eye', img_bgr)
            
            global_renderer.update_scene(data_mj, camera="global_cam")
            cv2.imshow('Global View', cv2.cvtColor(global_renderer.render(), cv2.COLOR_RGB2BGR))
            
            if cv2.waitKey(1) == 27: break

    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()